{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cas12 Screening for self-processing pre-crRNAs v1.0\n",
    "\n",
    "`Input`: Cas12 proteins in .fasta/.faa format.\n",
    "\n",
    "`Output`: predicted wether the Cas12 candidates would self-process their pre-crRNAs.\n",
    "\n",
    "*Note: before executing the following commands, please ensure that you have to save/add the following folders into your Google Drive (\"MyDrive\" folder):\n",
    "\n",
    "- [model](https://drive.google.com/drive/folders/1y4WKwsoBsqBb_R2Cdj0cwYiLIPnBXj01?usp=sharing)\n",
    "\n",
    "- [inputs](https://drive.google.com/drive/folders/18GGlIEWYtJVTn2oBXMqbghyYQCLKelLg?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step.01 setup **Environment** (~2m)\n",
    "%%time\n",
    "import os, time, signal\n",
    "import sys, random, string, re\n",
    "## mount google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "## install conda\n",
    "!pip install -q condacolab\n",
    "import condacolab\n",
    "condacolab.install()\n",
    "!conda update -n base -c conda-forge conda -y\n",
    "\n",
    "## packages install\n",
    "!pip install biopython\n",
    "!pip install git+https://github.com/facebookresearch/esm.git\n",
    "!conda install -c conda-forge pytorch_geometric -y\n",
    "!conda install -c conda-forge biotite -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### Models ###################\n",
    "\n",
    "import os, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import EsmModel, AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.backends.cudnn as cudnn\n",
    "import glob\n",
    "import esm\n",
    "import Bio.PDB\n",
    "import random\n",
    "import argparse\n",
    "\n",
    "## Unet model\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(mid_channels),\n",
    "            #nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(mid_channels),\n",
    "            #nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, bilinear=False):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        #self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 32)\n",
    "        self.down1 = Down(32, 64)\n",
    "        self.down2 = Down(64, 128)\n",
    "        self.down3 = Down(128, 256)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(256, 512 // factor)\n",
    "        self.up1 = Up(512, 256 // factor, bilinear)\n",
    "        self.up2 = Up(256, 128 // factor, bilinear)\n",
    "        self.up3 = Up(128, 64 // factor, bilinear)\n",
    "        self.up4 = Up(64, 32, bilinear)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.conv2 = nn.Conv2d(32, 2, kernel_size=1,bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.size())\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        # se模块\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "## Models\n",
    "\n",
    "class ModelLasthidden(nn.Module):     ## for CLS classification\n",
    "    def __init__(self, args):\n",
    "        super(ModelLasthidden, self).__init__()\n",
    "\n",
    "        self.dense = nn.Linear(args.emb_dim, args.emb_dim)\n",
    "        self.out_proj = nn.Linear(args.emb_dim, 2)\n",
    "        self.dropout = nn.Dropout(args.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x[:, :, 0, :]\n",
    "        x = x.squeeze(1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        y = self.out_proj(x)\n",
    "\n",
    "        return y\n",
    "\n",
    "class ModelLastHiddenMatrix(nn.Module):     ## for embedding classification\n",
    "    def __init__(self, args):\n",
    "        super(ModelLastHiddenMatrix, self).__init__()\n",
    "        self.unet = UNet(1)\n",
    "        self.am_pool = nn.AdaptiveMaxPool2d((16,16))\n",
    "        self.fc1 = nn.Linear(256*2, 64)\n",
    "        self.fc2 = nn.Linear(64,2)\n",
    "        self.dropout = nn.Dropout(args.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x[:,:,1:-1,:]\n",
    "        x = self.unet(x)\n",
    "        x = self.am_pool(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        y = self.fc2(x)\n",
    "        return y\n",
    "\n",
    "class ModelDist(nn.Module):     ## for distance map or esm_if classification\n",
    "    def __init__(self, args):\n",
    "        super(ModelDist, self).__init__()\n",
    "        self.unet = UNet(1)\n",
    "        self.am_pool = nn.AdaptiveMaxPool2d((16,16))\n",
    "        self.fc1 = nn.Linear(256*2, 64)\n",
    "        self.fc2 = nn.Linear(64,2)\n",
    "        self.dropout = nn.Dropout(args.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.unet(x)\n",
    "        x = self.am_pool(x)\n",
    "        x = x.view(x.size(0),-1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        y = self.fc2(x)\n",
    "        return y\n",
    "\n",
    "class MyDatasetEmbDist(Dataset):\n",
    "\n",
    "    def __init__(self, fea_path,datatype='embedding'):\n",
    "        self.fea_path = fea_path\n",
    "        self.file_list = sorted(glob.glob(fea_path+'/*.npy'))\n",
    "        self.datatype = datatype\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        name = os.path.basename(self.file_list[index])\n",
    "        if self.datatype == 'embedding':\n",
    "          feature = torch.from_numpy(np.load(os.path.join(self.fea_path, name))).float()\n",
    "        elif self.datatype == 'structure':\n",
    "          feature = np.load(os.path.join(self.fea_path, name))\n",
    "          feature = torch.unsqueeze(torch.from_numpy(feature).float(), 0)\n",
    "        return feature, name[:-4]\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### Data Processing ###################\n",
    "\n",
    "def data_gener_esm(layer,model,outdir,indir=None):\n",
    "    output_path = os.path.join(outdir,'the_%d_layer' % layer)\n",
    "    try:\n",
    "        os.makedirs(output_path)\n",
    "    except:\n",
    "        print('Output directory existed: %s' % output_path)\n",
    "    if indir==None:\n",
    "        file_list = glob.glob('protein'+'/*.faa')\n",
    "    else:\n",
    "        file_list = glob.glob(indir+'/*.faa')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    model = EsmModel.from_pretrained(model).to('cuda')\n",
    "\n",
    "    for f in file_list:\n",
    "        lines = open(f, 'r').readlines()\n",
    "        for i in range(len(lines)):\n",
    "            if lines[i][0] == '>':\n",
    "                seq = lines[i+1].strip()\n",
    "                inputs = tokenizer(seq, return_tensors=\"pt\").to('cuda')\n",
    "                outputs = model(output_hidden_states=True,return_dict=True,**inputs)\n",
    "                this_hidden_states = outputs.hidden_states[layer]\n",
    "                np.save(os.path.join(output_path, lines[i].strip()[1:]), this_hidden_states.detach().cpu().numpy())\n",
    "\n",
    "def calc_residue_dist(residue_one, residue_two,ca='CA') :\n",
    "    \"\"\"Returns the C-alpha distance between two residues\"\"\"\n",
    "    diff_vector  = residue_one[ca].coord - residue_two[ca].coord\n",
    "    return np.sqrt(np.sum(diff_vector * diff_vector))\n",
    "\n",
    "def calc_dist_matrix(chain_one, chain_two,ca='CA') :\n",
    "    \"\"\"Returns a matrix of C-alpha distances between two chains\"\"\"\n",
    "    answer = np.zeros((len(chain_one), len(chain_two)),'float')\n",
    "    for row, residue_one in enumerate(chain_one) :\n",
    "        for col, residue_two in enumerate(chain_two) :\n",
    "            answer[row, col] = calc_residue_dist(residue_one, residue_two, ca)\n",
    "    return answer\n",
    "\n",
    "def data_gener_from_pdb(outdir,mode='esmif',indir=None):\n",
    "    ## output directory\n",
    "    try:\n",
    "        os.makedirs(outdir)\n",
    "    except:\n",
    "        print('Output directory existed: %s' % outdir)\n",
    "\n",
    "    ## input pdbs\n",
    "    if indir==None:\n",
    "        pdb_list = glob.glob('protein'+'/*.pdb')\n",
    "    else:\n",
    "        pdb_list = glob.glob(indir+'/*.pdb')\n",
    "\n",
    "    ## esmfold_if\n",
    "    if mode == 'esmif':\n",
    "        ## load esmfold_if\n",
    "        model, alphabet = esm.pretrained.esm_if1_gvp4_t16_142M_UR50()   ## convert pdb structure to embedding (dim=512)\n",
    "        model = model.eval().cuda()\n",
    "        for this_pdb in pdb_list:\n",
    "            name = re.sub('\\.pdb$','',os.path.basename(this_pdb))\n",
    "            ## converting\n",
    "            chain_id = 'A'\n",
    "            structure = esm.inverse_folding.util.load_structure(this_pdb, chain_id)\n",
    "            coords, native_seq = esm.inverse_folding.util.extract_coords_from_structure(structure)\n",
    "            rep = esm.inverse_folding.util.get_encoder_output(model, alphabet, coords)\n",
    "            ## save\n",
    "            np.save(os.path.join(outdir,name),rep.cpu().detach().numpy())\n",
    "    ## distance map\n",
    "    elif mode == 'distance':\n",
    "        for this_pdb in pdb_list:\n",
    "            name = re.sub('\\.pdb$','',os.path.basename(this_pdb))\n",
    "            structure = Bio.PDB.PDBParser().get_structure(name, this_pdb)\n",
    "            model = structure[0]\n",
    "            dist_matrix = calc_dist_matrix(model[\"A\"], model[\"A\"],ca='CA')\n",
    "            np.save(os.path.join(outdir,name),dist_matrix)\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step.02 run **Self-processing Prediction** (~1m)\n",
    "%%time\n",
    "#@markdown **Parameters** settings\n",
    "#@markdown ---\n",
    "args = argparse.ArgumentParser(description='SelfProcess')\n",
    "datatype = 'embedding'\n",
    "model = \"CLS_best\" #@param [\"CLS_best\",\"CLS_lastlayer\",\"Embedding_best\",\"ESM_inverfold\",\"DistanceMap\"] {type:\"string\"}\n",
    "if model == 'CLS_best':\n",
    "  _model = 'ModelLasthidden'\n",
    "  _test_model = 'drive/MyDrive/models/spCas/esm2_650M_10th_cls.pt'\n",
    "elif model == 'CLS_lastlayer':\n",
    "  _test_model = 'drive/MyDrive/models/spCas/esm2_650M_last_cls.pt'\n",
    "elif model == 'Embedding_best':\n",
    "  _model = 'ModelLastHiddenMatrix'\n",
    "  _test_model = 'drive/MyDrive/models/spCas/esm2_650M_23th_emb.pt'\n",
    "elif model == 'ESM_inverfold':\n",
    "  _model = 'ModelDist'\n",
    "  _test_model = 'drive/MyDrive/models/spCas/esm_if.pt'\n",
    "  datatype = 'structure'\n",
    "elif model == 'DistanceMap':\n",
    "  _model = 'ModelDist'\n",
    "  _test_model = 'drive/MyDrive/models/spCas/distance_map.pt'\n",
    "  datatype = 'structure'\n",
    "\n",
    "args.model = _model\n",
    "args.test_model = _test_model\n",
    "\n",
    "feature_path = 'features'\n",
    "args.fea_path = feature_path\n",
    "try:\n",
    "  os.makedirs(args.fea_path)\n",
    "except:\n",
    "  pass\n",
    "\n",
    "from google.colab import files\n",
    "input_faa = 'drive/MyDrive/inputs/suspicious.faa' #@param {type:\"string\"}\n",
    "#@markdown - input protein sequence `.faa` for \"CLS_best\", \"CLS_lastlayer\", and \"Embedding_best\" modles.\n",
    "#@markdown - or input your own `.faa` file.\n",
    "input_pdb = 'drive/MyDrive/inputs/suspicious.pdb' #@param {type:\"string\"}\n",
    "#@markdown - input protein sequence `.pdb` for \"ESM_inverfold\" and \"DistanceMap\" modles.\n",
    "#@markdown - or input your own `.pdb` file.\n",
    "emb_dim = 1280 #@param [\"1280\"] {type:\"raw\"}\n",
    "#@markdown - embedding size for ESM-2 `esm2_t33_650M_UR50D`.\n",
    "seed = 42 #@param [\"42\"] {type:\"raw\"}\n",
    "#@markdown - ramdom seed. default `42`.\n",
    "out_table = 'sp_result.csv' #@param {type:\"string\"}\n",
    "#@markdown - output table of prediction.\n",
    "\n",
    "args.emb_dim = emb_dim\n",
    "args.seed = seed\n",
    "args.out_tab = out_table\n",
    "args.dropout = 0.2\n",
    "if model=='CLS_best' or model=='CLS_lastlayer' or model=='Embedding_best':\n",
    "  if input_faa:\n",
    "    !cp {input_faa} ./\n",
    "  else:\n",
    "    upload = files.upload()\n",
    "    input_faa = list(upload.keys())[0]\n",
    "  if model=='CLS_best':\n",
    "    data_gener_esm(10,'facebook/esm2_t33_650M_UR50D',args.fea_path,'./')\n",
    "    args.fea_path = args.fea_path + '/the_10_layer'\n",
    "    trained_model = ModelLasthidden(args).to(device)\n",
    "  elif model=='CLS_lastlayer':\n",
    "    data_gener_esm(33,'facebook/esm2_t33_650M_UR50D',args.fea_path,'./')\n",
    "    args.fea_path = args.fea_path + '/the_33_layer'\n",
    "    trained_model = ModelLasthidden(args).to(device)\n",
    "  elif model=='Embedding_best':\n",
    "    data_gener_esm(23,'facebook/esm2_t33_650M_UR50D',args.fea_path,'./')\n",
    "    args.fea_path = args.fea_path + '/the_23_layer'\n",
    "    trained_model = ModelLastHiddenMatrix(args).to(device)\n",
    "else:\n",
    "  if input_pdb:\n",
    "    !cp {input_pdb} ./\n",
    "  else:\n",
    "    upload = files.upload()\n",
    "    input_pdb = list(upload.keys())[0]\n",
    "  trained_model = ModelDist(args).to(device)\n",
    "  if model=='ESM_inverfold':\n",
    "    data_gener_from_pdb(args.fea_path,'esmif','./')\n",
    "  elif model=='DistanceMap':\n",
    "    data_gener_from_pdb(args.fea_path,'distance','./')\n",
    "\n",
    "## Prediciton\n",
    "trained_model.load_state_dict(torch.load(args.test_model), strict=False)\n",
    "test_set = MyDatasetEmbDist(args.fea_path,'embedding' )\n",
    "test_loader = DataLoader(test_set, 1, shuffle=None, num_workers=1)\n",
    "trained_model.eval().cuda()\n",
    "\n",
    "preds, probs, names = [], [], []\n",
    "with torch.no_grad():\n",
    "  for fea, name in test_loader:\n",
    "    fea = fea.to(device)\n",
    "    layer = nn.Softmax(dim=1)\n",
    "    output = layer(trained_model(fea))\n",
    "    preds.append(torch.argmax(output,dim=1).cpu().numpy())\n",
    "    probs.append(output[0][1].item())\n",
    "    names.append(name)\n",
    "\n",
    "data = {\n",
    "\"names\":[item[0] for item in names],\n",
    "\"preds\":[item[0] for item in preds],\n",
    "\"probs\":probs\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(os.path.join(out_table),sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['names','preds','probs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step.03 Package and download results\n",
    "from google.colab import files\n",
    "!zip -r CasPrediction.zip {out_table}\n",
    "files.download(f\"spCasPrediction.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Note*\n",
    "\n",
    "This pipeline is designed to screen for Cas12 candidates, identifying effectors capable of self-processing their own pre-crRNAs.\n",
    "\n",
    "We offer five modes of prediction:\n",
    "\n",
    "- `CLS_best`. Utilizes the 10th layer output CLS as the input to a classifier.\n",
    "\n",
    "- `CLS_lastlayer`. Utilized the last layer output CLS as the input to a classifier.\n",
    "\n",
    "- `Embedding_best`. Utilizes the 23th layer output embedding as the input to a classifier.\n",
    "\n",
    "- `ESM_inverfold`. Utilized the inverse-folding embedding of a structure `.pdb` as the input to a classifier.\n",
    "\n",
    "- `DistanceMap`. Utilized the distance map matrices of a structure `.pdb` as the input to a classifier.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "The output table `sp_result.csv` presents the following columns:\n",
    "\n",
    "- `names`. The ids of each protein.\n",
    "\n",
    "- `preds`. Model predicted labels.\n",
    "\n",
    "- `probs`. The normalized probability of this protein to be identified as self-processing its own pre-crRNAs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
