{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CasDiscovery Prediction v1.0\n",
    "\n",
    "`Input`: suspicious proteins in .fasta/.faa format.\n",
    "\n",
    "`Output`: predicted tags of the proteins, including `cas9`, `cas12`, `cas13` and `other`.\n",
    "\n",
    "*Note: before executing the following commands, please ensure that you have to save/add the following folders into your Google Drive (\"MyDrive\" folder):\n",
    "\n",
    "- [model](https://drive.google.com/drive/folders/1y4WKwsoBsqBb_R2Cdj0cwYiLIPnBXj01?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step.01 setup **Environment** (~6m)\n",
    "%%time\n",
    "import os, time, signal\n",
    "import sys, random, string, re\n",
    "## mount google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "## install conda\n",
    "!pip install -q condacolab\n",
    "import condacolab\n",
    "condacolab.install()\n",
    "!conda update -n base -c conda-forge conda -y\n",
    "\n",
    "## packages install\n",
    "!pip install pyfaidx\n",
    "\n",
    "## protein to be predicted\n",
    "!mkdir protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### prediction codes ###################\n",
    "import ast\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyfaidx\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from transformers import AutoTokenizer, EsmForSequenceClassification\n",
    "\n",
    "## Class\n",
    "class TqdmLoggingHandler(logging.Handler):\n",
    "    def __init__(self, level=logging.NOTSET):\n",
    "        super().__init__(level)\n",
    "\n",
    "    def emit(self, record):\n",
    "        try:\n",
    "            msg = self.format(record)\n",
    "            tqdm.write(msg)\n",
    "            self.flush()\n",
    "        except Exception:\n",
    "            self.handleError(record)\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, inputs, labels, names):\n",
    "        self.input_ids = inputs['input_ids']\n",
    "        self.attention_mask = inputs['attention_mask']\n",
    "        self.labels = torch.tensor(labels)\n",
    "        self.names = names\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    def __getitem__(self, idx):\n",
    "        return {'labels': self.labels[idx], 'input_ids': self.input_ids[idx], 'attention_mask': self.attention_mask[idx], 'ids': idx}\n",
    "    def get_num_samples_per_class(self):\n",
    "        return torch.bincount(self.labels).tolist()\n",
    "\n",
    "## Functions\n",
    "def parse_head_mask(value):\n",
    "    try:\n",
    "        print(torch.tensor(ast.literal_eval(value)))\n",
    "        return torch.tensor(ast.literal_eval(value))\n",
    "    except ValueError:\n",
    "        raise argparse.ArgumentTypeError(f\"Invalid head_mask value: {value}\")\n",
    "\n",
    "def create_dataset(tokenizer, fasta_dir, max_seq_len, label_to_id_fn, random_seed):\n",
    "    labels, sequences, names = read_fasta(fasta_dir)\n",
    "    if random_seed is not None:\n",
    "        labels, sequences, names = shuffle(labels, sequences, names, random_state=random_seed) # type: ignore\n",
    "    inputs = tokenizer(sequences, padding=True, truncation=True, max_length=max_seq_len, return_tensors='pt', add_special_tokens=True)\n",
    "    label_ids = label_to_id_fn(labels)\n",
    "    return SequenceDataset(inputs, label_ids, names)\n",
    "\n",
    "def read_fasta(fasta_dir):\n",
    "    labels = []\n",
    "    names = []\n",
    "    sequences = []\n",
    "    for fasta_file in os.listdir(fasta_dir):\n",
    "        if not fasta_file.endswith(('.faa', '.fasta')):\n",
    "            continue\n",
    "        label = fasta_file.split('.')[0]\n",
    "        fasta = pyfaidx.Fasta(os.path.join(fasta_dir, fasta_file), rebuild=False)\n",
    "        for record in fasta:\n",
    "            labels.append(label)\n",
    "            seq = str(record)\n",
    "            seq = re.sub(r\"[\\n\\*]\", '', seq)\n",
    "            seq = re.sub(r\"[UZOB]\", \"X\", seq)\n",
    "            sequences.append(seq)\n",
    "            names.append(record.name)\n",
    "    print(f\"Read {len(labels)} sequences from {fasta_dir}, sequences: {len(sequences)}, names: {len(names)} from fasta_dir: {fasta_dir}\")\n",
    "    time.sleep(1) # avoid multi process issues\n",
    "    return labels, sequences, names\n",
    "\n",
    "def get_dataloader(tokenizer, label_to_id_fn, random_seed, args):\n",
    "    eval_dataset = create_dataset(tokenizer, args.eval_dataset_dir, args.max_seq_len, label_to_id_fn, random_seed)\n",
    "    eval_dataloader = DataLoader(eval_dataset, batch_size=args.eval_batch_size)\n",
    "    return eval_dataloader\n",
    "\n",
    "def get_label_to_id_fn(all_labels):\n",
    "    def label_to_id_fn(labels):\n",
    "        return [all_labels.index(label) if label in all_labels else 0 for label in labels]\n",
    "    return label_to_id_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step.02 run **CasDiscovery Prediction** (~1m)\n",
    "%%time\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(TqdmLoggingHandler())\n",
    "\n",
    "DATASET_TRAINING_KEYS = ['labels', 'input_ids', 'attention_mask']\n",
    "\n",
    "#@markdown **Parameters** settings\n",
    "#@markdown ---\n",
    "\n",
    "from google.colab import files\n",
    "protein_dir = 'protein'\n",
    "input_protein = 'suspicious.faa' #@param {type:\"string\"}\n",
    "#@markdown - input protein `.faa` file. default `suspicious.faa`.\n",
    "#@markdown - or you can upload your own protein `.faa` file.\n",
    "if input_protein == 'suspicious.faa':\n",
    "  !cp drive/MyDrive/inputs/suspicious.faa protein/\n",
    "else:\n",
    "  input_protein = files.upload()\n",
    "  input_protein = list(input_protein.keys())[0]\n",
    "  !mv {input_protein} protein/\n",
    "\n",
    "max_seq_len = 1560 #@param [\"1560\"] {type:\"raw\"}\n",
    "#@markdown - maximum length of your protein sequence. default `1560`.\n",
    "batch_size = 1 #@param [\"1\"] {type:\"raw\"}\n",
    "random_seed = 42 #@param [\"42\"] {type:\"raw\"}\n",
    "\n",
    "out_table = 'pred_result.csv' #@param {type:\"string\"}\n",
    "#@markdown - output table.\n",
    "\n",
    "args = argparse.ArgumentParser(description='CasDiscovery')\n",
    "args.eval_dataset_dir = protein_dir\n",
    "args.max_seq_len = max_seq_len\n",
    "args.eval_batch_size = batch_size\n",
    "model_name = 'drive/MyDrive/models/CasDiscovery'\n",
    "all_labels = ['cas9','cas12','cas13','noncas']\n",
    "dataset_random_seed = None\n",
    "\n",
    "label_to_id_fn = get_label_to_id_fn(all_labels)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = EsmForSequenceClassification.from_pretrained(model_name, num_labels=len(all_labels)).cuda().eval()\n",
    "eval_dataloader = get_dataloader(tokenizer, label_to_id_fn, dataset_random_seed, args)\n",
    "\n",
    "merged_ids = torch.tensor([], dtype=torch.int)\n",
    "merged_predicted_label_ids = torch.tensor([], dtype=torch.int)\n",
    "merged_logits = torch.tensor([], dtype=torch.float)\n",
    "\n",
    "with torch.no_grad():\n",
    "  for batch in eval_dataloader:\n",
    "    inputs = {k: v for k, v in batch.items() if k in DATASET_TRAINING_KEYS}\n",
    "    label_ids = batch.get(\"labels\")\n",
    "    ids = batch.get(\"ids\")\n",
    "    inputs['labels'] = inputs['labels'].cuda()\n",
    "    inputs['input_ids'] = inputs['input_ids'].cuda()\n",
    "    inputs['attention_mask'] = inputs['attention_mask'].cuda()\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.get(\"logits\")\n",
    "    predicted_probs = torch.softmax(logits, dim=1)\n",
    "    predicted_label_ids = torch.argmax(logits, dim=1)\n",
    "\n",
    "    merged_ids = torch.cat((merged_ids, ids.cpu()), dim=0)\n",
    "    merged_predicted_label_ids = torch.cat((merged_predicted_label_ids, predicted_label_ids.cpu()), dim=0)\n",
    "    merged_logits = torch.cat((merged_logits, predicted_probs.cpu()), dim=0)\n",
    "\n",
    "    merged_predicted_labels = [all_labels[label_id] for label_id in merged_predicted_label_ids]\n",
    "    merged_names = [eval_dataloader.dataset.names[id] for id in merged_ids]\n",
    "    df = pd.DataFrame({\n",
    "      \"name\": merged_names,\n",
    "      \"predicted_label\": merged_predicted_labels,\n",
    "    })\n",
    "    for i, label in enumerate(all_labels):\n",
    "      df[f\"prob: {label}\"] = [f\"{round(prob[i] * 100, 2)}%\" for prob in merged_logits.numpy()]\n",
    "    df.to_csv(out_table, sep='\\t',index=False)\n",
    "\n",
    "print('Prediction finished!\\nResults output to: %s' % out_table)\n",
    "print('------------------------------')\n",
    "print('Total protein: %d' % df.shape[0])\n",
    "print('Cas9: %d' % df[df.predicted_label=='cas9'].shape[0])\n",
    "print('Cas12: %d' % df[df.predicted_label=='cas12'].shape[0])\n",
    "print('Cas13: %d' % df[df.predicted_label=='cas13'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['name','predicted_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Step.03 Package and download results\n",
    "from google.colab import files\n",
    "!zip -r CasPrediction.zip {out_table}\n",
    "files.download(f\"CasPrediction.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Note*\n",
    "\n",
    "This pipeline use the fine-tuned ESM-2 `facebook/esm2_t33_650M_UR50D` for `EsmForSequenceClassification`. When you got suspicious proteins abjacent to the CRISPR arrays, this pipeline would help you to identify the potential single effector Cas emzymes (`cas9`, `cas12` or `cas13`).\n",
    "\n",
    "The output table `pred_result.csv` presents the following columns:\n",
    "\n",
    "- `name`. The ids of each protein.\n",
    "\n",
    "- `predicted_label`. Model predicted labels.\n",
    "\n",
    "- `prob: cas9`. The normalized probability of this protein to be identified as a Cas9 enzyme.\n",
    "\n",
    "- `prob: cas12`. The normalized probability of this protein to be identified as a Cas12 enzyme.\n",
    "\n",
    "- `prob: cas13`. The normalized probability of this protein to be identified as a Cas13 enzyme.\n",
    "\n",
    "- `prob: noncas`. The normalized probability of this protein to be identified as a non-single effector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
